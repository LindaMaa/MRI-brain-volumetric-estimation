{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Osxq7F5xlm",
        "outputId": "7ef54d7b-4dc3-4ff9-cd80-3ffefce26bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0xMh6Zz6A1H",
        "outputId": "e986fe0f-5112-454f-c973-f4418d051288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial shapes\n",
            "(321, 40, 128, 128)\n",
            "(321, 40, 128, 128)\n",
            "(321,)\n",
            "(185, 40, 128, 128)\n",
            "(185, 40, 128, 128)\n",
            "(185,)\n",
            "(71, 40, 128, 128)\n",
            "(71, 40, 128, 128)\n",
            "(71,)\n",
            "Check after split\n",
            "torch.Size([430, 40, 128, 128])\n",
            "torch.Size([430, 40, 128, 128])\n",
            "torch.Size([76, 40, 128, 128])\n",
            "torch.Size([76, 40, 128, 128])\n",
            "torch.Size([71, 40, 128, 128])\n",
            "torch.Size([71, 40, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# load subcortical data\n",
        "X_Guys = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/X_Guys.npy')\n",
        "y_Guys = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/y_Guys.npy')\n",
        "ids_Guys = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/ids_Guys.npy')\n",
        "X_HH = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/X_HH.npy')\n",
        "y_HH = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/y_HH.npy')\n",
        "ids_HH = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/ids_HH.npy')\n",
        "X_IOP = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/X_IOP.npy')\n",
        "y_IOP = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/y_IOP.npy')\n",
        "ids_IOP = np.load('/content/drive/MyDrive/dhl_exam/data/subcortical/ids_IOP.npy')\n",
        "\n",
        "#center data\n",
        "def centring(X):\n",
        "    X=np.asarray(X)\n",
        "    epsilon = 1e-7 \n",
        "    mean = np.mean(X, axis=0, keepdims=True)\n",
        "    std = np.std(X, axis=0, keepdims=True)\n",
        "    centered_array = (X - mean) / (std+epsilon)\n",
        "    return centered_array\n",
        "\n",
        "X_Guys_centered=centring(X_Guys)\n",
        "X_HH_centered=centring(X_HH)\n",
        "X_IOP_centered=centring(X_IOP)\n",
        "\n",
        "# producing required train / val / test split\n",
        "print(\"initial shapes\")\n",
        "print(X_Guys.shape)\n",
        "print(y_Guys.shape)\n",
        "print(ids_Guys.shape)\n",
        "print(X_HH.shape)\n",
        "print(y_HH.shape)\n",
        "print(ids_HH.shape)\n",
        "print(X_IOP.shape)\n",
        "print(y_IOP.shape)\n",
        "print(ids_IOP.shape)\n",
        "\n",
        "combined_Guys_HH_X = np.concatenate([X_Guys_centered, X_HH_centered], axis=0)\n",
        "combined_Guys_HH_y = np.concatenate([y_Guys, y_HH], axis=0)\n",
        "\n",
        "X_train = torch.Tensor(combined_Guys_HH_X[0:int(len(combined_Guys_HH_X)*0.85)])\n",
        "y_train = torch.Tensor(combined_Guys_HH_y[0:int(len(combined_Guys_HH_y)*0.85)])\n",
        "\n",
        "# val data 15%\n",
        "X_val = torch.Tensor(combined_Guys_HH_X[int(len(combined_Guys_HH_X)*0.85):int(len(combined_Guys_HH_X))])\n",
        "y_val = torch.Tensor(combined_Guys_HH_y[int(len(combined_Guys_HH_y)*0.85):int(len(combined_Guys_HH_y))])\n",
        "#test data from IOP data\n",
        "X_test =  torch.Tensor(X_IOP_centered[0:int(len(X_IOP_centered))])\n",
        "y_test = torch.Tensor(y_IOP[0:int(len(y_IOP))])\n",
        "\n",
        "print(\"Check after split\")\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "TGN8ElOhWc8L",
        "outputId": "d79d280f-f081-4bf9-eca7-f88024eeb82c"
      },
      "outputs": [],
      "source": [
        "print(\"Inspecting Guys data\")\n",
        "print(X_Guys.shape)\n",
        "print(y_Guys.shape)\n",
        "print(ids_Guys.shape)\n",
        "print(\"Inspecting HH data\")\n",
        "print(X_HH.shape)\n",
        "print(y_HH.shape)\n",
        "print(ids_HH.shape)\n",
        "print(\"Inspecting IOP data\")\n",
        "print(X_IOP.shape)\n",
        "print(y_IOP.shape)\n",
        "print(ids_IOP.shape)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "\n",
        "axes[0, 0].imshow(X_Guys[:][10][18])\n",
        "axes[0, 0].set_title(\"Guys X\")\n",
        "\n",
        "axes[0, 1].imshow(X_HH[:][10][18])\n",
        "axes[0, 1].set_title(\"HH X\")\n",
        "\n",
        "axes[0, 2].imshow(X_IOP[:][10][18])\n",
        "axes[0, 2].set_title(\"IOP X\")\n",
        "\n",
        "axes[1, 0].imshow(y_Guys[:][10][18])\n",
        "axes[1, 0].set_title(\"Guys y\")\n",
        "\n",
        "axes[1, 1].imshow(y_HH[:][10][18])\n",
        "axes[1, 1].set_title(\"HH y\")\n",
        "\n",
        "axes[1, 2].imshow(y_IOP[:][10][18])\n",
        "axes[1, 2].set_title(\"IOP y\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_LShKhU6Tzo",
        "outputId": "76d29b1d-b2c0-4e53-94e4-1cf97e1ecb7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check shapes after slicing into 2D\n",
            "torch.Size([430, 1, 128, 128])\n",
            "torch.Size([430, 1, 128, 128])\n",
            "torch.Size([76, 1, 128, 128])\n",
            "torch.Size([76, 1, 128, 128])\n",
            "torch.Size([71, 1, 128, 128])\n",
            "torch.Size([71, 1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# slice data into 2D\n",
        "def reslice(x, split_size):\n",
        "  temp = torch.split(x,split_size,dim=1)\n",
        "  output=[]\n",
        "  for i in range(len(temp)):\n",
        "    output.append(temp[i])\n",
        "  output=torch.cat(output, dim=0)\n",
        "  return output\n",
        "\n",
        "#X_train=reslice(X_train,1)\n",
        "#y_train=reslice(y_train,1)\n",
        "#X_val=reslice(X_val,1)\n",
        "#y_val=reslice(y_val,1)\n",
        "#X_test=reslice(X_test,1)\n",
        "#y_test=reslice(y_test,1)\n",
        "\n",
        "#sample slices - just pick 1 slice from 40 so for training always \n",
        "X_train=X_train[:,10,:,:]\n",
        "X_val=X_val[:,20,:,:]\n",
        "X_test=X_test[:,30,:,:]\n",
        "y_train=y_train[:,10,:,:]\n",
        "y_val=y_val[:,20,:,:]\n",
        "y_test=y_test[:,30,:,:]\n",
        "\n",
        "#introduce channel\n",
        "X_train=torch.reshape(X_train,(len(X_train),1,128,128))\n",
        "X_val=torch.reshape(X_val,(len(X_val),1,128,128))\n",
        "X_test=torch.reshape(X_test,(len(X_test),1,128,128))\n",
        "y_train=torch.reshape(y_train,(len(y_train),1,128,128))\n",
        "y_val=torch.reshape(y_val,(len(y_val),1,128,128))\n",
        "y_test=torch.reshape(y_test,(len(y_test),1,128,128))\n",
        "\n",
        "\n",
        "print(\"Check shapes after slicing into 2D\")\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y3l-Aqx6XCq",
        "outputId": "9e3e82d5-c4bc-469f-9019-507fc850ed5f"
      },
      "outputs": [],
      "source": [
        "# convert y to one_hot\n",
        "\n",
        "def to_one_hot(y, num_classes):\n",
        "    y = np.array(y, dtype='int') - 1  \n",
        "    one_hot = np.eye(num_classes)[y.flatten()]\n",
        "    return one_hot.reshape(y.shape[0], num_classes, y.shape[2], y.shape[3]).astype(float)\n",
        "\n",
        "y_train = to_one_hot(y_train, 5)\n",
        "y_val = to_one_hot(y_val, 5)\n",
        "y_test = to_one_hot(y_test, 5)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzgwdLm_6bQ8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "X_train=np.asarray(X_train)\n",
        "X_val=np.asarray(X_val)\n",
        "X_test=np.asarray(X_test)\n",
        "\n",
        "class numpy_dataset(Dataset): \n",
        "    def __init__(self, data, target): \n",
        "        self.data =  torch.from_numpy(data)\n",
        "        self.target = torch.from_numpy(target)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "train_dataset = numpy_dataset(X_train, y_train)\n",
        "val_dataset = numpy_dataset(X_val, y_val)\n",
        "test_dataset = numpy_dataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62jXYg1n6igA"
      },
      "outputs": [],
      "source": [
        "class SegNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(512)\n",
        "        self.conv7 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.conv8 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(128)\n",
        "        self.conv9 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(64)\n",
        "        self.conv10 = nn.Conv2d(64, num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x, pool1_indices = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x, pool2_indices = self.pool(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x, pool3_indices = self.pool(x)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x, pool4_indices = self.pool(x)\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x, pool5_indices = self.pool(x)\n",
        "        x = self.unpool(x, pool5_indices)\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.unpool(x, pool4_indices)\n",
        "        x = F.relu(self.bn7(self.conv7(x)))\n",
        "        x = self.unpool(x, pool3_indices)\n",
        "        x = F.relu(self.bn8(self.conv8(x)))\n",
        "        x = self.unpool(x, pool2_indices)\n",
        "        x = F.relu(self.bn9(self.conv9(x)))\n",
        "        x = self.conv10(x)\n",
        "        x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class FCN_flexible(nn.Module):\n",
        "    def __init__(self, input_shape=(1, 128, 128), num_classes=2, dropout_prob=0.5, num_layers=5):\n",
        "        super(FCN_flexible, self).__init__()\n",
        "        encoder_layers = []\n",
        "        in_channels = input_shape[0]\n",
        "        for i in range(num_layers):\n",
        "            out_channels = 64 * (2 ** i)\n",
        "            encoder_layers.extend([\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout_prob),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            ])\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        self.middle = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1)\n",
        "        )\n",
        "        decoder_layers = []\n",
        "        for i in range(num_layers - 1, -1, -1):\n",
        "            out_channels = 64 * (2 ** i)\n",
        "            decoder_layers.extend([\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout_prob)\n",
        "            ])\n",
        "            in_channels = out_channels\n",
        "\n",
        "        decoder_layers.extend([\n",
        "            nn.Conv2d(out_channels, num_classes, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        ])\n",
        "\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def conv_block(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv = conv_block(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(self.pool(x))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = conv_block(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat((skip, x), dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        self.init_conv = conv_block(in_channels, 64)\n",
        "        self.encoders = nn.ModuleList([\n",
        "            Encoder(64, 128),\n",
        "            Encoder(128, 256),\n",
        "            Encoder(256, 512),\n",
        "            Encoder(512, 1024)\n",
        "        ])\n",
        "        self.decoders = nn.ModuleList([\n",
        "            Decoder(1024, 512),\n",
        "            Decoder(512, 256),\n",
        "            Decoder(256, 128),\n",
        "            Decoder(128, 64)\n",
        "        ])\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.init_conv(x)\n",
        "        skips = [x1]\n",
        "        for encoder in self.encoders:\n",
        "            skips.append(encoder(skips[-1]))\n",
        "\n",
        "        x = skips.pop()\n",
        "        for decoder in self.decoders:\n",
        "            x = decoder(x, skips.pop())\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGVCzDnB6nDp"
      },
      "outputs": [],
      "source": [
        "# training loops adapted from Kaggle \n",
        "def dice_coef_metric(pred, label):\n",
        "    intersection = 2.0 * (pred * label).sum()\n",
        "    union = pred.sum() + label.sum()\n",
        "    if pred.sum() == 0 and label.sum() == 0:\n",
        "        return 1.\n",
        "    return intersection / union\n",
        "\n",
        "def dice_coef_loss(pred, label):\n",
        "    smooth = 1.0\n",
        "    intersection = 2.0 * (pred * label).sum() + smooth\n",
        "    union = pred.sum() + label.sum() + smooth\n",
        "    return 1 - (intersection / union)\n",
        "\n",
        "def bce_dice_loss(pred, label):\n",
        "    dice_loss = dice_coef_loss(pred, label)\n",
        "    bce_loss = nn.BCELoss()(pred, label)\n",
        "    return dice_loss + bce_loss\n",
        "\n",
        "def train_loop(model, loader, loss_func,optimizer):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_dices = []\n",
        "\n",
        "    for i, (image, mask) in enumerate(loader):\n",
        "        image = image.to(device).float()\n",
        "        mask = mask.to(device).float()\n",
        "        outputs = model(image)\n",
        "        out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "        out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
        "        out_cut[np.nonzero(out_cut >= 0.5)] = 1.0            \n",
        "\n",
        "        dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n",
        "        loss = loss_func(outputs, mask)\n",
        "        train_losses.append(loss.item())\n",
        "        train_dices.append(dice)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return train_dices, train_losses\n",
        "\n",
        "\n",
        "def train_model_early_stopping(train_loader, val_loader, loss_func,optimizer, scheduler, num_epochs, patience=5):\n",
        "    train_loss_history = []\n",
        "    train_dice_history = []\n",
        "    val_loss_history = []\n",
        "    val_dice_history = []\n",
        "\n",
        "    best_val_dice = 0\n",
        "    consecutive_no_improvement = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_dices, train_losses = train_loop(model, train_loader, loss_func, optimizer)\n",
        "        train_mean_dice = np.array(train_dices).mean()\n",
        "        train_mean_loss = np.array(train_losses).mean()\n",
        "        val_mean_dice, val_mean_loss = eval_loop(model, val_loader, loss_func,scheduler)\n",
        "        \n",
        "        train_loss_history.append(train_mean_loss)\n",
        "        train_dice_history.append(train_mean_dice)\n",
        "        val_loss_history.append(val_mean_loss.cpu().numpy())\n",
        "        val_dice_history.append(val_mean_dice)\n",
        "        \n",
        "        print('Epoch: {}/{} |  Train Loss: {:.3f}, Val Loss: {:.3f}, Train DICE: {:.3f}, Val DICE: {:.3f}'.format(epoch+1, num_epochs,\n",
        "                                                                                                                 train_mean_loss,\n",
        "                                                                                                                 val_mean_loss,\n",
        "                                                                                                                 train_mean_dice,\n",
        "                                                                                                                 val_mean_dice))\n",
        "        \n",
        "        # Check for improvement in validation dice coefficient\n",
        "        if val_mean_dice > best_val_dice:\n",
        "            best_val_dice = val_mean_dice\n",
        "            consecutive_no_improvement = 0\n",
        "            print('Best validation dice coefficient improved to {:.3f}'.format(best_val_dice))\n",
        "        else:\n",
        "            consecutive_no_improvement += 1\n",
        "            print('No improvement in validation dice coefficient for {} consecutive epochs'.format(consecutive_no_improvement))\n",
        "            if consecutive_no_improvement >= patience:\n",
        "                print('Early stopping triggered after {} epochs'.format(epoch+1))\n",
        "                break\n",
        "\n",
        "    return train_loss_history, train_dice_history, val_loss_history, val_dice_history,epoch+1\n",
        "\n",
        "def eval_loop(model, loader, loss_func, scheduler,training=True):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_dice = 0\n",
        "    with torch.no_grad():\n",
        "        for step, (image, mask) in enumerate(loader):\n",
        "            image = image.to(device).float()\n",
        "            mask = mask.to(device).float()\n",
        "    \n",
        "            outputs = model(image)\n",
        "            loss = loss_func(outputs, mask)\n",
        "            \n",
        "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
        "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
        "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
        "            dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n",
        "            \n",
        "            val_loss += loss\n",
        "            val_dice += dice\n",
        "        \n",
        "        val_mean_dice = val_dice / len(loader)\n",
        "        val_mean_loss = val_loss / step\n",
        "        \n",
        "        if training:\n",
        "            scheduler.step(val_mean_dice)\n",
        "        \n",
        "    return val_mean_dice, val_mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z1yaxlK7EQa"
      },
      "outputs": [],
      "source": [
        "def prediction_dice(net, test_dataloader):\n",
        "    test_dice=0\n",
        "\n",
        "    with torch.no_grad():  \n",
        "        for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "          data = data.to(device).float()\n",
        "          target = target.to(device).float()\n",
        "           \n",
        "          pred = net(data)\n",
        "          out_cut = np.copy(pred.data.cpu().numpy())\n",
        "          out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
        "          out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
        "          dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n",
        "          test_dice += dice\n",
        "        mean_dice = test_dice / len(test_dataloader)\n",
        "        return mean_dice\n",
        "\n",
        "def predict(net, test_dataloader):\n",
        "    test_dice=0\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for batch_idx, (data, target) in enumerate(test_dataloader):\n",
        "          data = data.to(device).float()\n",
        "          target = target.to(device).float()\n",
        "           \n",
        "          pred = net(data)\n",
        "    return data.data.cpu().numpy(),target.data.cpu().numpy(),pred.data.cpu().numpy()\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-QDj8kxZnFA",
        "outputId": "cf0dedc1-b98c-472b-c5ae-b7a551196f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/100 |  Train Loss: 1.286, Val Loss: 1.617, Train DICE: 0.186, Val DICE: 0.249\n",
            "Best validation dice coefficient improved to 0.249\n",
            "Epoch: 2/100 |  Train Loss: 1.082, Val Loss: 1.379, Train DICE: 0.355, Val DICE: 0.487\n",
            "Best validation dice coefficient improved to 0.487\n",
            "Epoch: 3/100 |  Train Loss: 0.504, Val Loss: 0.285, Train DICE: 0.871, Val DICE: 0.988\n",
            "Best validation dice coefficient improved to 0.988\n",
            "Epoch: 4/100 |  Train Loss: 0.143, Val Loss: 0.124, Train DICE: 0.984, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 5/100 |  Train Loss: 0.089, Val Loss: 0.087, Train DICE: 0.985, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 6/100 |  Train Loss: 0.077, Val Loss: 0.079, Train DICE: 0.985, Val DICE: 0.989\n",
            "No improvement in validation dice coefficient for 1 consecutive epochs\n",
            "Epoch: 7/100 |  Train Loss: 0.070, Val Loss: 0.069, Train DICE: 0.985, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 8/100 |  Train Loss: 0.067, Val Loss: 0.066, Train DICE: 0.985, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 9/100 |  Train Loss: 0.065, Val Loss: 0.066, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 1 consecutive epochs\n",
            "Epoch: 10/100 |  Train Loss: 0.065, Val Loss: 0.066, Train DICE: 0.985, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 11/100 |  Train Loss: 0.065, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 12/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 1 consecutive epochs\n",
            "Epoch: 13/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "Best validation dice coefficient improved to 0.990\n",
            "Epoch: 14/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 1 consecutive epochs\n",
            "Epoch: 15/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 2 consecutive epochs\n",
            "Epoch: 16/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 3 consecutive epochs\n",
            "Epoch: 17/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 4 consecutive epochs\n",
            "Epoch: 18/100 |  Train Loss: 0.064, Val Loss: 0.065, Train DICE: 0.985, Val DICE: 0.990\n",
            "No improvement in validation dice coefficient for 5 consecutive epochs\n",
            "Early stopping triggered after 18 epochs\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet(n_channels=1, n_classes=5, bilinear=True).to(device)\n",
        "print(\"Number of parameters: \", count_parameters(model))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
        "start_time = time.time()\n",
        "train_loss_history, train_dice_history, val_loss_history, val_dice_history = train_model_early_stopping(train_dataloader, val_dataloader, bce_dice_loss, optimizer, scheduler, num_epochs)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq0wTSmhoVUK",
        "outputId": "a3e22408-69a0-4cb3-d6b4-e3a39eb66ee6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9309134400427358"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_dice(model, test_dataloader) #unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQxeOs5Iq4sC"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SegNet(1, 5).to(device)\n",
        "print(\"Number of parameters: \", count_parameters(model))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
        "start_time = time.time()\n",
        "train_loss_history, train_dice_history, val_loss_history, val_dice_history = train_model_early_stopping(train_dataloader, val_dataloader, bce_dice_loss, optimizer, scheduler, num_epochs)\n",
        "end_time = time.time()\n",
        "print(\"time taken\", end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYLyauVR1aua",
        "outputId": "13c67a2a-fd37-4ef9-9b4e-bb3e29693b8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8644542985718776"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_dice(model, test_dataloader) #segnet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
